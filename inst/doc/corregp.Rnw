%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{Correspondence regression: A tutorial}

\documentclass{article}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0.6em plus 0.75em minus 0.5em}

\begin{document}

\title{Correspondence regression: A tutorial}
\author{Koen Plevoets}
\maketitle

<<Setup, include = FALSE>>=
knitr::opts_chunk$set(highlight = FALSE, size = "small")
@

\section{By way of introduction: A bird's eye view of correspondence
regression}

Correspondence regression rests on the idea, described by
Gilula and Haberman (1988), of modelling a multi-category response variable
in terms of several (categorical) explanatory variables. Consider
the built-in data set (in R)\\* \texttt{HairEyeColor}, which gives the
distribution of 592  students with respect to their hair color, eye color and
sex:

<<HairEyeColor>>=
HairEyeColor
@

A similar data table (with other subjects and without the \texttt{Sex}
variable) was used by Fisher (1940), where he laid the foundations of
the technique of \emph{correspondence analysis} (together with Hirschfeld
1935). Although Fisher was primarily concerned with the association between
hair color categories and eye color categories, one could see \texttt{Eye}
color as a response variable and \texttt{Hair} color and \texttt{Sex} as two
explanatory variables. A geneticist, for instance, might well be interested
in predicting the color of people's iris (i.e. their eye color) on the basis
of their hair color and sex. Correspondence regression is meant for such an
analysis.

The \texttt{HairEyeColor} data set is also contained in the \textbf{corregp}
package, where it is reshaped into the data frame \texttt{HairEye} (and
some of the labels have also been renamed):

<<HairEye, warning = FALSE>>=
library(corregp)
data(HairEye)
summary(HairEye)
ftable(HairEye, col.vars = "Eye")
@

The `flat' contingency table (produced with the \texttt{ftable()} function)
nicely illustrates how the distribution of the \texttt{Eye} color categories
can be studied in function of the (combination of) the \texttt{Hair} color
categories and the \texttt{Sex} categories.

The package \textbf{corregp} has a single function for correspondence
regression: the eponymous \texttt{corregp()}. The name of this function is
a reference to the function \texttt{corresp()} from the package
\textbf{MASS} (Venables and Ripley 2002) for simple correspondence analysis,
and \texttt{corregp()} shares some of the computational features of
\texttt{corresp()}. In line with the goal of regressing a response variable
on explanatory variables, \texttt{corregp()} takes a typical R formula as
its first argument: e.g. \texttt{Eye \~{} Hair * Sex} performs a
correspondence regression of the response variable \texttt{Eye} in function
of the (combination of the) two explanatory variables \texttt{Hair} and
\texttt{Sex} (more specifically, it performs a correspondence regression of
\texttt{Eye} in function of the main effect of \texttt{Hair + }the main
effect of \texttt{Sex + }plus the interaction between \texttt{Hair} and
\texttt{Sex}, i.e. \texttt{Hair:Sex}). For more details on R formulas, see
\texttt{help(formula)} or Section 2.1. Note that all variables will
automatically be treated as categorical (i.e. R `factors'): if you specify a
numeric variable somewhere, then \texttt{corregp()} will convert it to a
categorical/factor variable! The data frame containing the (categorical)
variables can be specified as a second argument. The \texttt{corregp()}
function contains many other arguments (see Section 2.1 for an overview, or
read the function's help page: \texttt{help(corregp)}), but one important
one is the argument \texttt{b} which specifies the number of bootstrap
replications (in fact, these are Monte Carlo simulations). This is relevant
if one wishes to study the inferential properties of the results, such as
confidence regions (the default value for \texttt{b} is \texttt{0}, which
leaves the analysis exploratory). See Appendix 2 for more details on the
inferential procedure. In the example below, we choose a random seed of
\texttt{12345} (with the function \texttt{set.seed()}) in order to make
the results reproducible (so, if you also copy-paste it, then you will
obtain the same confidence regions as are printed in this tutorial):

<<CRG>>=
set.seed(12345)
haireye.crg <- corregp(Eye ~ Hair * Sex, data = HairEye, b = 3000)
summary(haireye.crg)
@

The summary of the correspondence regression gives some overall statistics
and it lists the distribution of the so-called `eigenvalues'. The
\texttt{Chi-squared} value is the same as Pearson's Chi-squared statistic
of the above-mentioned `flat' contingency table (which you can test
yourself: type in\\*
\texttt{chisq.test(ftable(HairEye, col.vars = "Eye"), correct = FALSE)}\\*
and compare the value in the \texttt{X-squared} field of the output). The
\texttt{Phi-squared} value is equal to the \texttt{Chi-squared} value
divided by \texttt{N}, the total number of observations. Both the
\texttt{Chi-squared} and the \texttt{Phi-squared} value express the
dependence between the response variable (\texttt{Eye}) and the (combined)
explanatory variables (\texttt{Hair} and \texttt{Sex}). The core idea behind
correspondence regression is the same as behind correspondence analysis (or
the \emph{correlation models} of Gilula and Haberman 1988): these techniques
assume that the response variable and the explanatory variables can be
modelled in terms of underlying, latent axes which \emph{explain} the
observed dependencies. The underlying axes can also be thought of as
\emph{latent variables}, and correspondence analysis calls them
`principal axes'. For instance, the first two axes underlying the
association between the \texttt{Eye} variable and the combination of the
\texttt{Hair} and \texttt{Sex} variables can be illustrated in the following
`association graph' (the details behind association graphs will be
explained later in this tutorial):

<<AgPlot, fig.align = 'center', out.width = '70%', echo = FALSE>>=
agplot(haireye.crg, axes = 1:2)
@

The eigenvalues in the summary indicate the `explanatory power' of each
principal or latent axis (correspondence analysis also speaks of
`principal inertias', which are the eigenvalues divided by \texttt{N}). They
are given in three ways: the first row (\texttt{value}) shows the actual
eigenvalues, the second row (\texttt{\%}) shows the relative values, and
the third row (\texttt{cum\textunderscore \%}) shows the cumulative
relative values. One sees that the sum of the actual eigenvalues is equal
to the \texttt{Chi-squared} value (the `principal inertias' of
correspondence analysis likewise sum to the \texttt{Phi-squared} value). The
interpretation is straightforward: the latent axes \emph{decompose} the
observed association (between response variable and explanatory variables)
into different sets.

Because we used the \texttt{summary()} function without any arguments, the
reported eigenvalues are descriptive measures of the data set (for precise
details on the computation of the latent axes, see standard textbooks on
correspondence analysis such as Greenacre 2017, or see Appendix 1). However,
since we have applied correspondence regression with bootstrapping (Monte
Carlo simulations), we can compute \emph{confidence intervals} for the
eigenvalues. The confidence intervals are printed by setting the argument
\texttt{add\textunderscore ci} to \texttt{TRUE}:

<<SumCi>>=
summary(haireye.crg, add_ci = TRUE)
@

For the actual eigenvalues (in \texttt{Value}), the percentages, or the
cumulative percentages, we see the observed measures together with a lower
confidence bound (in \texttt{lower}) and an upper confidence bound (in
\texttt{upper}). For example, the confidence interval for the first
eigenvalue is $[96.30 ; 165.60]$, the confidence interval for the second
eigenvalue is $[1.98 ; 33.34]$, the confidence interval for the first
\emph{relative} eigenvalue is $[0.76 ; 0.96]$, etc. By default, these are
95\% confidence intervals, but you can specify the confidence level yourself
with the argument \texttt{cl} (see \texttt{help(summary.corregp)}).

It is the philosophy of the \textbf{corregp} package that as many results as
possible can be visualized. The eigenvalues can therefore be plotted in a
`scree plot' together with their confidence intervals. The \textbf{corregp}
package has the function \texttt{screeplot} which also has the argument
\texttt{add\textunderscore ci}:

<<Scree, fig.align = 'center', out.width = '65%'>>=
screeplot(haireye.crg, add_ci = TRUE)
@

On the basis of the eigenvalues it can be determined which latent axes are
`important'. As mentioned, the eigenvalues measure the observed association
between response and explanatory variables, and the results always sort the
eigenvalues from the largest value to the smallest one. Hence, the first
eigenvalues indicate important or informative axes, whereas the last
eigenvalues indicate uninformative ones. Because any data set is always a
random sample, these uninformative axes can be considered as reflecting
the `sampling noise' in the data. In practice, one inspects the eigenvalues
for a certain cutoff point between the informative and the noisy axes. In a
scree plot one typically looks for an `elbow' among the eigenvalues: the
first few eigenvalues generally exhibit a sharp decline with large
differences between successive eigenvalues, but after a while the majority of
the information (i.e. association) has been `explained', so the differences
between the later successive eigenvalues are not so large anymore. Such a
point can be said to represent an elbow in the scree plot. The scree plot for
the \texttt{HairEye} data contains only a few latent axes, so an elbow is
not so easy to discern, but one could claim that there exists one at the
second axis. That means that the first two latent axes are the informative
ones. Because the eigenvalues are descriptive statistics of the data, such
a statement is not based on statistical inference. However, if we have
computed confidence intervals for the eigenvalues, then we can use them to
back up our decision. In the output of the \texttt{summary()} function or in
the scree plot, we see that the \texttt{lower} limit of the second
eigenvalue is \emph{lower} than the \texttt{upper} limit of the third
eigenvalue. In other words, the confidence interval of the second eigenvalue
overlaps with the confidence interval of the third eigenvalue, so the
difference between the two eigenvalues can be regarded as not statistically
significant (incidentally, the lower limit of the third eigenvalue is a
negative value, which means that \texttt{0} lies within the confidence
interval or that the third eigenvalue is not significantly different from
\texttt{0}). The conclusion is that the confidence intervals of the
eigenvalues also indicate that the informative latent axes are the first
two.

Once we have decided to retain two latent axes, we can inspect the results
by visually plotting them in a two-dimensional coordinate space. Because
correspondence regression is based on correspondence regression, the
typical two-dimensional display is a `biplot', which shows the categories
of both the response variable and the explanatory variables. In the
\textbf{corregp} package, the biplot is made with the generic function
\texttt{plot()}, which has many arguments for customizing the outlook (see
\texttt{help(plot.corregp)}):

<<Biplot, fig.align = 'center', out.width = '65%'>>=
plot(haireye.crg, x_ell = TRUE, xsub = c("Hair", "Sex"))
@

The distances in the plot are (inverse) reflections of the associations
between the categories. For example, the eye color
\texttt{Brown\textunderscore E} is highly associated with the hair color
\texttt{Black}, as are the eye color \texttt{Blue} and the hair color
\texttt{Blond}. Similarly, the hair color \texttt{Red} is associated with
both the eye colors \texttt{Hazel} and \texttt{Green}, just as the hair
color \texttt{Brown\textunderscore H} (in the center of the plot) appears to
be indiscriminate between the eye colors \texttt{Brown\textunderscore E} and
\texttt{Hazel}. Other interpretations can be read off from the plot in the
same vein. Because we asked for confidence regions for the explanatory
variables (with the argument \texttt{x\textunderscore ell}), the plot
exhibits a two-dimensional `confidence ellipse' for every category of
\texttt{Hair} and \texttt{Sex} (the computation of the confidence
ellipses is done by a call to the \texttt{cell()} function; see
\texttt{help(cell.corregp)}). As a consequence, we are able to see that the
difference between the hair colors \texttt{Black} and \texttt{Blond} (in
the distribution of the eye colors) is statistically significant, both are
significantly different from \texttt{Brown\textunderscore H} and
\texttt{Red}, but the latter are not significantly different from each
other. Similarly, there is no statistically significant difference between
the sexes \texttt{Female} and \texttt{Male} (again, with respect to the
distribution of the eye colors). A corollary of regressing a response
variable on multiple explanatory variables is that the categories of
\emph{different} explanatory variables can also be compared with each
other. For instance, we see that the sex category \texttt{Female} is not
significantly different from the hair colors
\texttt{Brown\textunderscore H} and \texttt{Red}, but it is significantly
different from \texttt{Black} or \texttt{Blond} (and the same holds for
\texttt{Male}).

The biplot above only contains the main effects of \texttt{Hair} and
\texttt{Sex} because of our use of the argument \texttt{xsub} (see
\texttt{help(plot)} or see Section 2.2 for more details on \texttt{ysub},
\texttt{xsub} or other arguments of \texttt{plot()}). We have left the
(eight) combination categories of the interaction variable
\texttt{Hair:Sex} out of the biplot for the sake of illustration, since
their inclusion would render the plot rather dense (they could be visualized
with \texttt{xsub = "Hair.Sex"}). However, it is a common practice in the
context of regression to first determine the relative importance of the
explanatory variables (in explaining the variation in the response variable)
before examining the effects. This is the \emph{regression} aspect of
correspondence regression: it is the analysis of how strongly associated
each explanatory variable is to the response variable. It is particularly
relevant if one has specified many different interactions in the
\texttt{formula} of the correspondence regression and one wants to find out
which are the important ones (so one could subsequently use the
\texttt{xsub} argument to select only those, for instance). The
\textbf{corregp} package has a function \texttt{anova()} which produces a
so-called `ANOVA table', i.e. for every predictor term in the correspondence
regression it lists the (explained) association with the response variable
which is not due to the other predictor terms. Association is measured by
means of the Pearson Chi-squared statistic, and if one uses the
\texttt{anova()} function without any specification of the number of
latent axes, then the values in the output are directly related to the
Pearson Chi-squared statistics of the cross table formed by each
predictor term with the response variable (the specification of the number
of latent axes is, of course, evident after the examination of the
eigenvalues and/or the scree plot, but we first want to describe the ANOVA
table):

<<Anova1>>=
anova(haireye.crg)
@

It can be easily verified that the value \texttt{138.289842} in (the first
column \texttt{X\^{}2} of) the output is the Pearson Chi-squared
statistic of the cross table of \texttt{Hair} and (the response variable)
\texttt{Eye}, while \texttt{1.529824} is the Pearson Chi-squared statistic
of the cross table of \texttt{Sex} and \texttt{Eye}, and \texttt{10.264820}
is the Pearson Chi-squared statistic of the cross table of \texttt{Hair:Sex}
and \texttt{Eye} \emph{minus} \texttt{138.289842} and \texttt{1.529824},
i.e. \texttt{150.0845} \texttt{=} \texttt{138.289842} \texttt{+}
\texttt{1.529824} \texttt{+} \texttt{10.264820}. In other words, the
value (\texttt{X\^{}2}) for \texttt{Hair.Sex} (in the third row) expresses
the amount of association that the interaction term \texttt{Hair:Sex}
exhibits with the response \texttt{Eye} which \emph{cannot} be explained by
the main predictors \texttt{Hair} and \texttt{Sex} together. This is the
typical way of analysing the `contribution' of every predictor term to the
explanation of the response variable in regression (the \texttt{X\^{}2}
values in the ANOVA table are in fact computed according to the `additive'
definition of interactions by Darroch 1974 and Kroonenberg and Anderson
2006).

However, the construction of an ANOVA table for correspondence
regression usually involves the specification of the number of
latent axes. This can be done with the argument \texttt{nf} (see
\texttt{help(anova.corregp)} for the other arguments of the \texttt{anova()}
function). From the discussion of the eigenvalues and the scree plot above
we know that the first two latent axes are the informative ones for
\texttt{haireye.crg}, so we build the ANOVA table for those two latent axes
(naturally, the \texttt{X\^{}2} values are somewhat reduced, since we have
omitted the information on the third latent axis):

<<Anova2>>=
anova(haireye.crg, nf = 2)
@

If the correspondence regression contains bootstrap
replications/simulations (like \texttt{haireye.crg}), then the
\texttt{anova()} function will also give confidence intervals for the
\texttt{X\^{}2} values, with which their statistical significance can be
assessed. A predicter term is significant (and important) if \texttt{0} lies
\emph{outside} of its confidence interval. In the ANOVA table of the
\texttt{HairEye} example, we see that the main effect of \texttt{Hair} as
well as the interaction \texttt{Hair:Sex} are statistically significant, but
the main effect of \texttt{Sex} is not. Apparently, the difference between
the two sexes is not an important predictor for (the response variable)
\texttt{Eye} color. This corroborates the result in the
biplot above, where the individual categories \texttt{Female} and
\texttt{Male} were found not to be significantly different from each
other (it also means that the correspondence regression \texttt{haireye.crg}
with one main effect and one interaction represents a so-called
\emph{non-hierarchical} model, but we will not pursue that issue further
here). The presence of non-significant predictor terms does not necessarily
imply the refitting of the correspondence regression, since the goodness
of fit is determined with respect to the (number of) latent axes. Only
when one wants to inspect new effects (or effects which were not included
in the \texttt{formula} before) can one rerun the correspondence
regression. However, the significant predictor terms in the ANOVA table bear
on the subsequent steps in the analysis, because they are the typical
effects that one wants to visualize and study. That means that the biplot
above should in fact have contained the effects of the interaction
\texttt{Hair:Sex}, i.e. by setting \texttt{xsub = "Hair.Sex"}. In the
remainder of this tutorial, we will keep using the main effects
\texttt{Hair} and \texttt{Sex} for the sake of illustration, although the
ANOVA table clearly points out that the interaction \texttt{Hair:Sex} is
more informative.

We continue this Introduction with some other plotting functionalities than
the biplot. Although two-dimensional plots are customary for correspondence
analysis, it is also possible to visualize the results of correspondence
regression for one dimension, three dimensions or more. As a matter of fact,
it is a general practice to use the (one-dimensional) confidence intervals
for a single latent axis in order to determine whether the score of a
particular category (on that latent axis) is significantly different from
\texttt{0} or not. The plot of the confidence intervals of the categories
on a certain latent axis (which you select) can be made with the function
\texttt{ciplot()} (which itself calls the \texttt{cint()} function in order
to compute the confidence intervals; see \texttt{help(cint.corregp)}). The
\texttt{ciplot()} function has two important arguments among many other
ones (see \texttt{help(ciplot.corregp)}): \texttt{parm} controls which
categories are plotted and \texttt{axis} specifies the latent axis. The
\texttt{parm} argument can be used very flexibly: the value \texttt{"y"}
will plot all the categories of the response variable, the value
\texttt{"x"} will plot all the categories of (all) the explanatory
variables, a character vector with \emph{names} of explanatory variables
will plot only the categories of the specified explanatory variables, or
finally, a character vector of \emph{category names} (i.e. `levels') will
plot only the specified categories. For example, the plot of the confidence
intervals of the response variable categories on the first latent axis is
obtained as follows:

<<Ci1, fig.align = 'center', out.width = '65%'>>=
ciplot(haireye.crg, parm = "y", axis = 1)
@

This can then be compared to the confidence intervals of the categories of
the explanatory variables (plots of confidence intervals for predictor
variables are of course quite common in regression analysis). As mentioned
above, we only illustrate the main categories of the \texttt{Hair} and
\texttt{Sex} variables (so, not the categories of the interaction variable
\texttt{Hair:Sex}):

<<Ci2, fig.align = 'center', out.width = '65%'>>=
ciplot(haireye.crg, parm = c("Hair", "Sex"), axis = 1)
@

We see that the score of the hair color \texttt{Blond} (on the first latent
axis) is significantly larger than \texttt{0}, the scores of both
\texttt{Black} and \texttt{Brown\textunderscore H} are significantly smaller
than \texttt{0}, but the scores of the hair color \texttt{Red} as well as
the two sexes \texttt{Female} and \texttt{Male} are \emph{not} significantly
different from \texttt{0} (similarly, the score of the eye color
\texttt{Blue} is significantly larger than \texttt{0}, the scores of both
\texttt{Black} and \texttt{Hazel} are significantly smaller than
\texttt{0} while the score of \texttt{Green} is \emph{not} significantly
different from \texttt{0}). The same tests can be done for all the other
latent axes as well as for the interaction \texttt{Hair:Sex}.

Because the \textbf{corregp} package imports the \textbf{rgl} package
(Adler, Murdoch et al. 2017), it is also possible to make three-dimensional
plots. This is of course not necessary for the \texttt{HairEye} data, where
two latent axes are sufficient, but we will discuss plots of more than two
dimensions for the sake of illustration. The function for 3D plots is
\texttt{plot3d()}, which has special arguments for correspondence
regression (see \texttt{help(plot3d.corregp)}). For example, we can ask for
the three-dimensional confidence ellipsoids for the explanatory variables
with the argument \texttt{x\textunderscore ell} (which calls the
\texttt{cell3d()} function for the 3D confidence ellipsoids; see
\texttt{help(cell3d.corregp)}), and with the argument \texttt{xsub}
we again visualize only the main categories of \texttt{Hair} and
\texttt{Sex} (if you run the following code statement in R, then you can
inspect the contents of the 3D plot by rotating it):

<<TriCode, eval = FALSE>>=
plot3d(haireye.crg, x_ell = TRUE, xsub = c("Hair", "Sex"))
@

<<TriPlot, include = FALSE, eval = FALSE>>=
par3d(viewport = c(-128, -128, 512, 512), zoom = 1.5)
plot3d(haireye.crg, x_ell = TRUE, xsub = c("Hair", "Sex"))
snapshot3d("Tri.png")
@

\begin{center}
  \includegraphics[width=0.55\linewidth]{Tri.png}
\end{center}

If you happen to have a data set for which the eigenvalues point out that
you need \emph{more} than three latent axes, then you have three options
(obviously, a single scatterplot of the results is no longer possible). A
straightforward solution is to make multiple use of \texttt{ciplot()},
\texttt{plot()} or \texttt{plot3d()} in order to visualize all the relevant
latent axes. Both the \texttt{plot()} and \texttt{plot3d()} function have
an \texttt{axes} argument with which you can select a combination of the
latent axes (see the help pages of both functions). For instance, if you
have an analysis with four important latent axes, then you can make four
one-dimensional plots with\\*
\texttt{ciplot(, axis = 1)}, 
\texttt{ciplot(, axis = 2)}, \texttt{ciplot(, axis = 3)} and\\*
\texttt{ciplot(, axis = 4)}, or you can generate all six two-dimensional
plots (or just a subset thereof) with \texttt{plot(, axes = c(1, 2))},
\texttt{plot(, axes = c(1, 3))},\\* \texttt{plot(, axes = c(1, 4))},
\texttt{plot(, axes = c(2, 3))},\\*
\texttt{plot(, axes = c(2, 4))} and  \texttt{plot(, axes = c(3, 4))}, or
you can even create (a subset of) the four three-dimensional plots with\\*
\texttt{plot3d(, axes = c(1, 2, 3))},
\texttt{plot3d(, axes = c(1, 2, 4))},\\*
\texttt{plot3d(, axes = c(1, 3, 4))} and
\texttt{plot3d(, axes = c(2, 3, 4))}. An alternative is to make a
`parallel coordinate plot', in which the latent axes are displayed
next to each other and the scores of individual categories are connected by
a line. The function for a parallel coordinate plot in the \textbf{corregp}
package is \texttt{pcplot()}, which again has many arguments for
customization (see \texttt{help(pcplot.corregp)}). For example, a parallel
coordinate plot of the \texttt{Eye} colors on the first three latent axes
can be obtained as follows (just as the 3D plot above, this plot is not
necessary for the \texttt{HairEye} data):

<<PCplot, fig.align = 'center', out.width = '65%'>>=
pcplot(haireye.crg, parm = "y", axes = 1:3)
@

Finally, you can visualize the results of a correspondence regression in an
association graph, such as the one on pages 3--4 in this tutorial.
Association graphs are \emph{directed acyclic graphs}, in which the
different latent axes are depicted as circles and the individual categories
of both the response variable and explanatory variables are depicted as
boxes. It is a convention to give the circles a white color (for continuous
variables) and the boxes a grey color (for discrete variables), but all
colors can be changed manually. An association graph draws an arrow from a
specific latent axis to a specific category \emph{if and only if} the score
of that category on that latent axis is significantly different from
\texttt{0}, i.e. \texttt{0} does not lie within the confidence interval of
that category on that latent axis (the confidence intervals are computed
with the \texttt{cint()} function). In other words, if no arrow is drawn
between a certain category and a certain latent axis, then this indicates
that the score of that category on that latent axis is \emph{not}
significantly different from \texttt{0}, i.e. it is effectively
\texttt{0}. The fact that the appearance of arrow between categories and
latent axes is based on statistical significance entails, by the way, that
association graphs can only be made if the correspondence regression
contains bootstrap replicates/simulations! The (two) functions for
visualizing association graphs in the \textbf{corregp} package are
\texttt{agplot()} and \texttt{plotag()} (see either
\texttt{help(agplot.corregp)} or \texttt{help(plotag.corregp)} for the
help page), which in turn make use of the functionalities in the package
\textbf{diagram} (Soetaert 2014). The association graph for the
\texttt{HairEye} data above, for instance, can be obtained as follows:

<<AgCode, eval = FALSE>>=
agplot(haireye.crg, axes = 1:2)
@

For a general examination of the association between response variable and
explanatory variables, the plots of the scores/coordinates are usually
informative enough (especially the biplot). However, one can also look
further into the relations between the latent axes and the individual
categories. That can be useful for finding an \emph{interpretation} for the
latent axes. There are two measures: the
\emph{contributions of the points to the axes} express how well each
category represents (the inertia corresponding to) a certain latent axis,
while the \emph{contributions of the axes to the points} express
how well each latent axis reflects a certain category. The difference
between both measures is essentially that the former contributions (i.e.
of the points to the axes) sum to 100\% for each latent axis, whereas
the latter contributions (i.e. of the axes to the points) sum to 100\% for
each individual category. The former contributions are sometimes also
called the `absolute contributions', and the latter contributions are
sometimes referred to as the `squared correlations'. Both contributions can
be consulted with the \texttt{summary()} function by specifying the argument
\texttt{contrib}. That argument can have a plethora of possible values: the
contributions of the points to the axes are given by
\texttt{"p\textunderscore a"}, \texttt{"pts\textunderscore axs"},
\texttt{"pts2axs"},  \texttt{"ptstoaxs"},
\texttt{"pts\textunderscore to\textunderscore axs"},
\texttt{"pnts\textunderscore axes"}, \texttt{"pnts2axes"},
\texttt{"pntstoaxes"} or
\texttt{"pnts\textunderscore to\textunderscore axes"}, the contributions of
the axes to the points are given by \texttt{"a\textunderscore p"},
\texttt{"axs\textunderscore pts"}, \texttt{"axs2pts"}, \texttt{"axstopts"},
\texttt{"axs\textunderscore to\textunderscore pts"},
\texttt{"axes\textunderscore pnts"}, \texttt{"axes2pnts"},
\texttt{"axestopnts"} or\\*
\texttt{"axes\textunderscore to\textunderscore pnts"}. Last but not least,
the \texttt{contrib} argument can also have the value \texttt{"both"} or
\texttt{"b"}. In the following example, we specify the first two latent
axes (with the argument \texttt{nf}), so the contributions of the axes to
the points (\texttt{"axes2pnts"}) are of course less than 100\%:

<<Contrib1>>=
summary(haireye.crg, parm = "y", contrib = "axes2pnts", nf = 2)
@

<<Contrib2>>=
summary(haireye.crg, parm = "x", contrib = "pts_axs", nf = 2)
@

In summary, correspondence regression typically involves the following
steps:
\begin{enumerate}
  \item Perform a correspondence regression with the \texttt{corregp()}
  function.
  \item Check the eigenvalues in order to determine the number of important
  latent axes. You can use either \texttt{summary()} or
  \texttt{screeplot()}.
  \item Build an ANOVA table with \texttt{anova()} in order to discern the
  important predictor terms among the explanatory variables.
  \item Visualize the results with \texttt{ciplot()}, \texttt{plot()},
  \texttt{plot3d()}, \texttt{agplot()} (or \texttt{plotag()}) or
  \texttt{pcplot()}. In case you need more than two latent axes, you can
  use \texttt{ciplot()} or \texttt{plot()} (and
  even \texttt{plot3d()}) several times and make different selections of
  the latent axes.
  \item If you want more information on the interpretation of the latent
  axes, then you can inspect the
  \emph{contributions of the point to the axes} and/or the
  \emph{contributions of the axes to the points} by specifying the
  \texttt{contrib} argument of \texttt{summary()}.
\end{enumerate}

\section{Functions and methods in the \texttt{corregp} package}

\subsection{The function \texttt{corregp()}}

Because \texttt{corregp()} is the central function for correspondence
regression, we will now explain each of its arguments (see also
\texttt{help(corregp)}):
\begin{description}
  \item[\texttt{formula}] \hspace{1em} This should be a typical R formula
  with a response variable on the left-hand side and explanatory variables
  on the right-hand side (separated by a tilde). See Section
  \emph{11.1 Defining statistical models; formulae} of the standard R
  manual \emph{An Introduction to R} for an elaborate description. Examples
  for correspondence regression are (for convenience' sake, we will denote
  the response variable as \texttt{Y} and the explanatory variables as
  \texttt{X1}, \texttt{X2} etc.):\begin{itemize}
  \item \texttt{Y \~{} X1} \hspace{1em} Correspondence regression of
  \texttt{Y} in function of \texttt{X1}. This is equivalent to a
  \emph{simple correspondence analysis} of the two variables \texttt{Y} and
  \texttt{X1}.
  \item \texttt{Y \~{} X1 + X2} \hspace{1em} Correspondence regression of
  \texttt{Y} in function of both \texttt{X1} and \texttt{X2}. The output
  will only contain the results for the main categories of \texttt{X1} and
  the main categories of \texttt{X2} (next to the main categories of
  \texttt{Y}).
  \item \texttt{Y \~{} X1 + X2 + X1:X2} \hspace{1em} Correspondence
  regression of \texttt{Y} in function of the interaction between
  \texttt{X1} and \texttt{X2}. In other words, the output will contain
  results for the main categories of \texttt{X1}, the main categories of
  \texttt{X2} and the combined categories of the interaction \texttt{X1:X2}.
  \item \texttt{Y \~{} X1 * X2} \hspace{1em} The same as
  \texttt{Y \~{} X1 + X2 + X1:X2}.
  \item \texttt{Y \~{} (X1 + X2 + X3)\^{}2} \hspace{1em} Correspondence
  regression of \texttt{Y} in function of all the two-way interactions
  between \texttt{X1}, \texttt{X2} and \texttt{X3}. This is equivalent to
  \texttt{Y \~{} X1 + X2 + X3 + X1:X3 + X1:X2 + X2:X3}.
  \item \texttt{Y \~{} (X1 + X2 + X3)\^{}2 - X2:X3} \hspace{1em}
  Correspondence regression of \texttt{Y} in function of all the two-way
  interactions between \texttt{X1}, \texttt{X2} and \texttt{X3}
  \emph{except} the one between \texttt{X2} and \texttt{X3}. This is
  equivalent to\\*
  \texttt{Y \~{} X1 + X2 + X3 + X1:X2 + X1:X3}.
  \item \texttt{Y \~{} X1 * X2 * X3 - X1:X2:X3} \hspace{1em} Correspondence
  regression of \texttt{Y} in function of all the \emph{two-way}
  interactions between \texttt{X1}, \texttt{X2} and \texttt{X3}. The term
  \texttt{X1 * X2 * X3} denotes all the possible combinations of
  \texttt{X1}, \texttt{X2} and \texttt{X3}, but the three-way interaction
  \texttt{X1:X2:X3} is excluded. In other words, this is equivalent to\\*
  \texttt{Y \~{} X1 + X2 + X3 + X1:X2 + X1:X3 + X2:X3} (hence, also to\\*
  \texttt{Y \~{} (X1 + X2 + X3)\^{}2}).
  \item \texttt{Y \~{} X1/X2} \hspace{1em} Correspondence regression of
  \texttt{Y} in function of \texttt{X2} \emph{nested with} \texttt{X1}. This
  is just the same as \texttt{Y \~{} X1 + X1:X2} (or as\\*
  \texttt{Y \~{} X1*X2 - X2}).
  \item \texttt{Y \~{} -1 + X1 * X2} \hspace{1em} The same as
  \texttt{Y \~{} X1 * X2}. Intercepts are not part of correspondence
  regression in the first place, so their exclusion in the formula 
  does not change anything.
  \item \texttt{Y \~{} 0 + X1 * X2} \hspace{1em} The same as
  \texttt{Y \~{} -1 + X1 * X2}, so also as\\* \texttt{Y \~{} X1 * X2}.
  \item \ldots
\end{itemize}
  \item[\texttt{data}] \hspace{1em} This should be the data frame
  containing all the variables in \texttt{formula} as columns. IMPORTANT to
  remember is that any numeric or logical variable will be converted to a
  factor (i.e. a categorical variable).
  \item[\texttt{part}] \hspace{1em} This can be a (character) vector of
  conditional variables for both the response variable and the explanatory
  variables: if specified, then a correspondence regression of the response
  variable and the explanatory variables will be performed \emph{given}
  these variables. More specifically, if we denote such conditional
  variables as \texttt{Z1}, \texttt{Z2} etc., then correspondence regression
  of \texttt{Y \~{} X1 + X2 +\ldots} will basically amount to an analysis of
  \\*
  \texttt{Y:(Z1:Z2:\ldots) \~{} X1:(Z1:Z2:\ldots) + X2:(Z1:Z2:\ldots) +
  \ldots}.\\*
  Note that \texttt{Z1}, \texttt{Z2} etc. have to be columns in
  \texttt{data}, and the correct notation of the argument is
  \texttt{part = c("Z1", "Z2", \ldots)}, i.e. \textbf{with} quotation
  marks. A possible example of a conditional variable is a
  \emph{grouping factor} for the categories of the response (\texttt{Y})
  variable (which is relevant for so-called \emph{lectometric} analyses in
  linguistics).
  \item[\texttt{b}] \hspace{1em} This can be set equal to the number of
  bootstrap replications (Monte Carlo simulations). Those are new samples
  which are generated by resampling the observed data set (with
  replication). The new, replicated/simulated samples lead to to new values
  for both the eigenvalues and the scores on the latent axes. These new
  values can be used (by the functions \texttt{cint()}, \texttt{cell()} and
  \texttt{cell3d()}) to construct confidence regions. Usually, the number of
  replications/simulations is chosen to be quite large (e.g. 3000). If set
  to \texttt{0}, then no replicate samples are generated, so no confidence
  regions can be computed.
  \item[\texttt{xep}] \hspace{1em} This argument stands for `x separate'. By
  default, the results for all predictor terms in \texttt{formula} are
  collected as separate components in a list. This also admits the
  construction of an ANOVA table for the predictor terms. However, if you
  want the results of the explanatory variables collected in one overall
  matrix, then you can set this argument to \texttt{FALSE}.
  \item[\texttt{std}] \hspace{1em} This argument specifies whether to
  standardize the latent axes or not. The unstandardized scores on the
  latent axes are also called the `principal coordinates', and the variance
  of each latent axis is the corresponding eigenvalue. The standardized
  scores (i.e. with variance \texttt{1}) are also called the
  `standard coordinates'.
  \item[\texttt{rel}] \hspace{1em} By default, correspondence analysis (and
  correspondence regression) computes scores for the row profiles and the
  column profiles, i.e. the rows of a frequency table divided by their row
  totals and the columns of the table divided by their column totals. You
  can set this argument to \texttt{FALSE} if you want to obtain scores for
  the Pearson residuals ($\frac{O-E}{\sqrt{E}}$) instead. Leave this
  argument untouched unless you know what you are doing.
  \item[\texttt{phi}] \hspace{1em} This argument specifies whether the
  eigenvalues in the output should sum to the \emph{Chi-squared} value or
  to the \emph{Phi-squared} value, which is the Chi-squared value divided
  by the number of observations (see Appendix 1). In accordance with the
  \texttt{corresp()} function in the package \textbf{MASS}, the default for
  this argument is the first option.
  \item[\texttt{chr}] \hspace{1em} If the \texttt{formula} contains some
  interaction terms, such as \texttt{X1:X2}, then the output will contain
  results for the combination of the categories/levels of \texttt{X1} with
  the categories/levels of \texttt{X2}. The \texttt{chr} argument specifies
  which character string will be used as the connector (or `separator') for
  the combinations. The default for this argument is to combine the
  categories/levels with a single dot.
  \item[\texttt{b\textunderscore scheme}] \hspace{1em} This argument
  specifes the sampling scheme for bootstrapping (Monte Carlo
  simulation). It must be either \texttt{multinomial} (the default) or
  \texttt{product-multinomial} (or a string matching one of these two
  values). Multinomial sampling takes the frequency table of the \texttt{Y}
  and the (combination of the) \texttt{X} categories as one overall
  multinomial variable and uses the cell frequencies as the probabilities
  for resampling. Product-multinomial sampling treats the combination
  categories of the \texttt{X} variables as separate observations which are
  each multinomially distributed for the \texttt{Y} categories. Accordingly,
  the (re)sampling probabilities of the \texttt{Y} categories are different
  for each combination category in \texttt{X}. Product-multinomial sampling
  is the typical sampling scheme for multicategory response data, but both
  sampling schemes often give the same results.
\end{description}

The output of the \texttt{corregp()} function is a list, the components of
which are described in the \texttt{Value} section of the help file
(\texttt{help(corregp)}). Each component can be accessed individually:

<<OutCRG>>=
is.list(haireye.crg)
names(haireye.crg)
haireye.crg$y
@

\subsection{The plotting functions}

The plotting functions \texttt{ciplot()}, \texttt{plot()},
\texttt{plot3d()}, \texttt{agplot()} (\texttt{plotag()}) and
\texttt{pcplot()} have many more arguments than the ones described in the
Introduction. The majority of these involve settings for the usual
graphical parameters such as color, font size, font type, line width, line
type, main title of the plot, subtitle of the plot, labels for the axes
and/or limits for the axes (and so on). See the help pages of the functions
for a full overview (\texttt{help(ciplot)},
\texttt{help(plot.corregp)}, \texttt{help(plot3d.corregp)},
\texttt{help(agplot)},\\* \texttt{help(pcplot)}). Probably the best way to
master these arguments is by trial.

The functions \texttt{plot()}, \texttt{plot3d()} and \texttt{agplot()} (or
\texttt{plotag()}) contain two arguments \texttt{ysub} and \texttt{xsub}
which require some further clarification. In the examples in the
Introduction, \texttt{xsub} was used with the names of explanatory
variables. That automatically selected the categories (i.e. `levels')
belonging to those variables. However, both \texttt{ysub} and \texttt{xsub}
can also be specified with indices for the individual categories
themselves. The following code, for instance, plots only the main Hair
category \texttt{Blond}, the main Sex category \texttt{Female} and the
interaction category \texttt{Blond.Female} from the \texttt{X} results (and
\texttt{ysub} would work in the same way):

<<SubChr, fig.align = 'center', out.width = '65%', size = 'footnotesize'>>=
plot(haireye.crg, x_ell = TRUE, xsub = c("Blond", "Female", "Blond.Female"))
@

Both arguments can also be given \emph{numeric} indices, but there is a
difference between \texttt{ysub} and \texttt{xsub}. Numeric indices for
\texttt{ysub} simply select the corresponding rows from the \texttt{Y}
table in the same way as character names do. Numeric indices for
\texttt{xsub}, however, depend on whether the \texttt{corregp()} output was
generated with the argument \texttt{xep} being \texttt{TRUE} or
\texttt{FALSE}. If \texttt{xep = TRUE}, then the results for \texttt{X}
form a list themselves and the numeric indices select the corresponding
components from that list, i.e. all the categories of the selected predictor
term(s). If \texttt{xep = FALSE}, then the results for \texttt{X} are all
contained in one overall matrix, and the numeric indices (again) select the
corresponding rows. In other words, there is no way of selecting individual
categories with numeric indices if \texttt{xep = TRUE} (one has to use
character values or rerun \texttt{corregp()} with
\texttt{xep = FALSE}). The following code illustrates the use of numeric 
indices by plotting the \texttt{Blue} and \texttt{Brown\textunderscore E}
Eye colors as well as all the categories of both the main predictor
\texttt{Hair} and the main predictor \texttt{Sex}:

<<SubNum, fig.align = 'center', out.width = '65%', size = 'footnotesize'>>=
plot(haireye.crg, x_ell = TRUE, xsub = c(1, 2), ysub = c(1, 2))
@

\subsection{The functions for confidence regions}

Although confidence regions in the \texttt{corregp} package are primarily
computed for visualization, there are also  functions which give the
actual numeric output. These are \texttt{cint()} for (one-dimensional)
confidence intervals, \texttt{cell()} for (two-dimensional) confidence
ellipses and \texttt{cell3d()} for (three-dimensional) confidence
ellipsoids. Normally, users do not have to call these functions directly (as
they are called by \texttt{ciplot()}, \texttt{plot()} and \texttt{plot3d()},
respectively), but one can do so if one has a certain use for them (see
\texttt{help(cint)}, \texttt{help(cell)} or \texttt{help(cell3d)}). For
instance, the confidence intervals of the four \texttt{Eye} colors on the
first latent axis are:

<<Cint>>=
cint(haireye.crg, parm = "y", axis = 1)
@

All three functions contain the argument \texttt{cl} which specifies the
confidence level for the confidence regions (the default value is the
conventional \texttt{0.95}). This is the percentage of areas which would
contain the true population value (i.e. of a certain score) if the sample
were repeated. Because \texttt{corregp()} works with bootstrap
replications (simulations), this means that the confidence level
\texttt{cl} specifies the percentage of the \texttt{b} replicate values (for
each score) used to construct the confidence region.

The \texttt{cint()} function also has the argument \texttt{nq}, which
specifies whether one wants to construct the confidence interval using the
normal distribution or not. The use of the normal distribution (with
\texttt{nq = TRUE}) means that the mean and the standard deviation of the
\texttt{b} replicate values are computed on the basis of which a confidence
interval is constructed under the normal distribution (i.e. by means of the
function \texttt{qnorm()}). If one does not use the normal distribution
(with \texttt{nq = FALSE}), then the confidence interval is obtained from
the \texttt{b} replicate values themselves by means of the
\texttt{quantile()} function (i.e. by choosing two actual replicate values
as the lower and upper limit which together represent the confidence level
\texttt{cl}). The computation of bootstrap confidence intervals is usually
done in the second `non-parametric' way, but the first option is available
as the (one-dimensional) counterpart of two-dimensional confidence
ellipses (made with \texttt{cell()}), because the latter are \emph{always}
constructed by means of the bivariate normal distribution. It probably needs
no clarification that normal confidence intervals and non-parametric
confidence intervals can sometimes be quite different. For completeness'
sake, it should be pointed out that both the \texttt{summary()} function and
the \texttt{screeplot()} function also contain the arguments \texttt{cl} and
\texttt{nq} for the confidence intervals of the \emph{eigenvalues}, where
they have the same meaning. All confidence intervals in the \textbf{corregp}
package are computed with the function \texttt{ci()} (see
\texttt{help(ci)}).

The \texttt{cell()} function does not have an argument \texttt{nq}, but it
does have an argument \texttt{np}. That argument specifies the number of
points to represent an ellipsis. These points are connected by lines in
the visualization to form the ellipsis. The representation of ellipses by a
series of \texttt{np} points is due to the fact that \texttt{corregp()}
makes use of the \textbf{ellipse} package (Murdoch and Chow 2013) to compute
the confidence ellipses (which is inspired by R code on
www.carme-n.org). The default number of points is \texttt{100}, which
usually gives good results. One can increase this value if one wishes a
better resolution, but that will lead to a longer computation time.

Finally, the \texttt{cell3d()} function makes use of the
\texttt{ellipse3d()} function from the \textbf{rgl} package (which itself
creates a so-called \texttt{mesh3d} object). See the help pages of the
\textbf{rgl} package for further information. The \texttt{cell3d()}
function has no additional arguments.

\subsection{The functions for extracting coefficients, fitted values or
residuals}

The \textbf{corregp} package also has functions for extracting
coefficients, fitted values and/or residuals from a correspondence
regression. These are conventially named \texttt{coefficients()},
\texttt{fitted.values()} and \texttt{residuals()} with their customary
abbreviations \texttt{coef()}, \texttt{fitted()} and \texttt{resid()}. The
coefficients of correspondence regression are essentially the coordinate
scores of the categories on the latent axes, so the function
\texttt{coefficients()} (or \texttt{coef()}) prints them in a matrix or a
vector. It is used with the argument \texttt{parm} for the selection of the
categories (i.e. \texttt{"y"} for all the \texttt{Y} categories/levels,
\texttt{"x"} for all the \texttt{X} categories/levels, a vector of any
variable/term names in \texttt{X}, a vector of any category/level names in
\texttt{X} or a vector of any category/level names in \texttt{Y}) and the
argument \texttt{axes} for the selection of the latent axes (see
\texttt{help(coefficients.corregp)}). The function \texttt{fitted()} (or
\texttt{fitted.values()}) makes use of the coordinate scores in
correspondence regression to compute predicted/expected frequencies for
every cell in the cross table of \texttt{X} by \texttt{Y}. It is used with
the argument \texttt{parm} (in the same way as before) and the argument
\texttt{nf} for the selection of the number of latent axes (see
\texttt{help(fitted.corregp)}). The function \texttt{residuals()} (or
\texttt{resid()}) is the complement to \texttt{fitted()} in that it
computes the difference between observed frequencies and predicted
frequencies for every cell in the cross table of \texttt{X} and
\texttt{Y} (in other words, for a certain number of latent axes, the sum of
the fitted frequencies and the residuals is equal to the observed
frequencies --- except when conditional variables have been specified in
\texttt{part}, because the computation does not take the associations with
these variables into account). The function is also used with the
arguments \texttt{parm} and \texttt{nf} (see
\texttt{help(residuals.corregp)}). Examples for these three functions are:

<<Coef>>=
coef(haireye.crg, parm = c("Hair", "Sex"), axes = 1:2)
@

<<Fitted>>=
fitted(haireye.crg, parm = c("Hair", "Sex"), nf = 2)
@

<<Resid>>=
resid(haireye.crg, parm = c("Hair", "Sex"), nf = 2)
@

\section*{Appendix 1: The computation of correspondence regression}

Correspondence regression starts from a frequency table formed by
crossing the response variable (\texttt{Y}) with all the possible
combinations of the explanatory variables (\texttt{X}). The latter is the
same as the highest-order interaction of all the explanatory variables, i.e.
if the \texttt{formula} is specified as e.g. \texttt{Y \~{} X1 + X2 + X3},
then correspondence regression crosstabulates \texttt{Y} with
\texttt{X1:X2:X3} (in the usual R notation). If conditional variables 
(\texttt{Z1}, \texttt{Z2}, \ldots) are specified in the argument
\texttt{part}, then correspondence regression constructs a three-way table
by crossing \texttt{Y} and \texttt{X} for every possible combination of the
conditional variables (i.e. the joint distribution of the conditional
variables). Correspondence regression then computes the
\emph{Pearson residuals} $\frac{O-E}{\sqrt{E}}$ of this table, where $E$ is
calculated according to the usual formula of \emph{conditional independence}
of \texttt{Y} and \texttt{X} given \texttt{Z} (if no conditional variable is
specified in \texttt{part}, then $E$ is simply calculated as the
\emph{mutual} independence of \texttt{Y} and \texttt{X}). The three-way
table is subsequently aggregated over the conditional variables with
weights $\frac{n_{+jk}}{n_{+j+}}$ for every \texttt{Y} level \emph{j} and
\texttt{Z} level \emph{k} and weights $\frac{n_{i+k}}{n_{i++}}$ for every
\texttt{X} level \emph{i} and \texttt{Z} level \emph{k}. The resulting
matrix (of \texttt{Y} versus \texttt{X}) measures the same association as
the three-way correspondence analysis in Section 3.2 of 
Van der Heijden et al. (1989) (who make use of Escofier's 1984 generalized
correspondence analysis). If \texttt{phi = FALSE}, then we denote this
matrix of (aggregated) Pearson residuals as $\mathsf{D}$. Otherwise, if
\texttt{phi = TRUE}, then we let $\mathsf{D}$ be the matrix of (aggregated)
Pearson residuals divided by $\sqrt{N}$ (i.e. divided by the square root of
the total number of observations).

Just as correspondence analysis, correspondence regression computes the
Singular Value Decomposition of $\mathsf{D}$:
$$
\mathsf{D} = \mathsf{USV^T}
$$
The matrix $\mathsf{U}$ contains scores/coordinates for the explanatory
(\texttt{X}) categories and the matrix $\mathsf{V}$ contains
scores/coordinates for the response (\texttt{Y}) categories, which both
depend on the value of the argument \texttt{rel}. The diagonal matrix
$\mathsf{S}$ contains the so-called \emph{singular values}, which are the
square roots of the eigenvalues. In other words, the eigenvalues are the
diagonal values of $\mathsf{S^2}$.

If \texttt{rel = FALSE}, then the matrices $\mathsf{U}$ and $\mathsf{V}$
contain the \emph{standardized} scores/\\*coordinates of the \texttt{X} and
\texttt{Y} categories, respectively. The matrix $\mathsf{U}$ has a row for
every possible combination in \texttt{X}, so the more general categories
(i.e. the main categories and/or the lower-order interactions) are
obtained by aggregating the appropriate rows of $\mathsf{U}$.
If \texttt{rel = TRUE}, then the \emph{standardized} scores/coordinates
are computed by dividing the rows of $\mathsf{U}$ and $\mathsf{V}$ by the
\emph{square roots of the corresponding total frequencies} of the
\texttt{X} and \texttt{Y} categories, respectively. More specifically, let
$\mathbf{r}$ be (the vector of) the total frequencies of all \texttt{X}
categories and let $\mathbf{c}$ be (the vector of) the total frequencies of
the \texttt{Y} categories (they are, of course, also the row and column
totals of the original cross table of \texttt{X} and \texttt{Y}). Then the
standardized scores/coordinates of the \texttt{X} categories are obtained
with $\mathrm{diag}\left(\frac{1}{\sqrt{\mathbf{r}}}\right)*\mathsf{U}$ and
the standardized scores/coordinates of the \texttt{Y} categories are
obtained with
$\mathrm{diag}\left(\frac{1}{\sqrt{\mathbf{c}}}\right)*\mathsf{V}$. Again,
the $\mathsf{U}$ matrix only contains rows for every possible combination of
the \texttt{X} variables, so the score/coordinate of a more general category
can be obtained by aggregating the appropriate rows of $\mathsf{U}$ as well
as summing the corresponding totals in $\mathbf{r}$ and multiplying these
two. In other words, if the aggregated rows in $\mathsf{U}$ for a certain
lower-order category be denoted as $\mathsf{U_+}$ and the corresponding
sum of the totals be denoted as $\mathbf{r_+}$, then the standardized
score/coordinate of the lower-order category is
$\mathrm{diag}\left(\frac{1}{\sqrt{\mathbf{r_+}}}\right)*\mathsf{U_+}$.

The scores/coordinates which are actually outputted finally also depend on
the argument \texttt{std} (as was explained in Sextion 2.1). If
\texttt{std = TRUE}, then the scores/coordinates in the output for both the
\texttt{X} and the \texttt{Y} categories are the \emph{standardized}
scores/coordinates, which were just discussed. If \texttt{std = FALSE},
then the output contains the so-called \emph{principal} scores/coordinates,
which are computed by multiplying the standardized scores/coordinates of
both \texttt{X} and \texttt{Y} by the matrix $\mathsf{S}$.

The contributions of the points to the axes as well as the contributions
from the axes to the points can also be obtained from the matrices
$\mathsf{U}$, $\mathsf{S}$ and $\mathsf{V}$. The contributions of the
points to the axes (i.e. the `absolute contributions') for the \texttt{X}
categories are in the columns of the matrix $\mathsf{U^2}$, which each sum
to 1. In other words, for each latent axis, the contributions of the
\texttt{X} categories to the axis are shown in the columns of
$\mathsf{U^2}$. Likewise, the contributions of the points to the axes of
the \texttt{Y} categories are in the columns of $\mathsf{V^2}$. The
contributions of the axes to the points (i.e. the `squared correlations')
involve some matrix multiplication, so that the contributions can be read
from the rows of the resulting matrices. For the \texttt{X} categories, the
contributions of the axes to the points are in the rows of:
$$
\mathsf{(US)^2}*\mathrm{diag}\left(\frac{1}{\sum_k(\mathsf{US})_{ik}^2}\right)
$$
For the \texttt{Y} categories, the contributions of the axes to the points
can be obtained in the same fashion as the rows of:
$$
\mathsf{(VS)^2}*\mathrm{diag}\left(\frac{1}{\sum_k(\mathsf{VS})_{jk}^2}\right)
$$

\section*{Appendix 2: The computation of confidence regions}

As has been mentioned multiple times in this tutorial, the
\texttt{corregp()} function is able to compute confidence regions for its
results by means of a bootstrap procedure; in particular, this is Monte
Carlo simulation. Bootstrapping/\\*Simulation involves generating many (i.e.
\texttt{b}) new, replicated samples by
\emph{sampling the observed data set with replacement} (the replicated
samples always have the same size \texttt{N} as the observed data
set). The details of the replication depend on the argument
\texttt{b\textunderscore scheme} of the \texttt{corregp()} function. The
default is to arrange the data in a frequency table (by crossing
the response variable \texttt{Y} with all combinations of the explanatory
variables in \texttt{X}, sometimes extended to a three-way frequency table
in case of conditional variables in \texttt{part}) and to take the individual
cell counts (divided by \texttt{N}) as the estimates of the cell
probabilities. These estimated cell probabilities and the sample total
\texttt{N} are passed as arguments to the built-in R function
\texttt{rmultinom()}, which generates \texttt{b} new random multinomial
data samples. These are the replications/simulations of the observed data
set. Because this approach treats the frequency table as one
overall multinomial variable, it is called \emph{multinomial} sampling.

The alternative is \emph{product-multinomial} sampling which creates the
same frequency table but which only uses the response categories in
\texttt{Y} as a multinomial variable. The combination categories of the
\texttt{X} variable(s) are treated as separate observations of that
multinomial variable. That means that the cell probabilities are
estimated for each combination category in \texttt{X} by dividing by
the total frequency of that combination category. Replication/simulation
is still subsequently done with the function \texttt{rmultinom()} in
order to get \texttt{b} random replicates of the original data table.

In either sampling scheme, each of the replicated tables lead to
bootstrap/\\*simulated replicates of
the matrices $\mathsf{U}$, $\mathsf{S}$ and $\mathsf{V}$. This is done, in
particular, by means of the \emph{partial bootstrap} procedure, which is
outlined in Alvarez et al. (2002; 2004; 2006) and Lebart (2004). The
partial bootstrap makes use of the fact that the formula for the Singular
Value Decomposition (see Appendix 1 above) can be rewritten into
formulas for $\mathsf{U}$, $\mathsf{S}$ and $\mathsf{V}$ themselves. By
substituting a specific bootstrap/simulated table for the observed table in
the application of these formulas, the bootstrap/simulated replicates of
the $\mathsf{U}$, $\mathsf{S}$ and $\mathsf{V}$ matrices can be obtained (in
other words, these are computed in the same way as supplementary points in
correspondence analysis). More specifically, denote the Pearson residuals
of a particular bootstrap/simulated table as $\mathsf{D^*}$. Then, the
bootstrap/simulated replicate of the matrix $\mathsf{U}$, which will be
denoted as $\mathsf{U^*}$, can be derived as follows:
$$
\mathsf{U^*} = \mathsf{D^*VS^{-1}}
$$
Similarly, the bootstrap/simulated replicate of $\mathsf{V}$, denoted as
$\mathsf{V^*}$, is obtained as:
$$
\mathsf{V^*} = \mathsf{D^{*T}US^{-1}}
$$
Finally, the bootstrap/simulated replicate of $\mathsf{S}$ (hence, of the
eigenvalues), denoted as $\mathsf{S^*}$, can be computed as:
$$
\mathsf{S^*} = \mathsf{U^TD^*V}
$$
If one repeats this for all the \texttt{b} bootstrap/simulated tables, then
one gets \texttt{b} replicates of the matrices $\mathsf{U^*}$,
$\mathsf{S^*}$ and $\mathsf{V^*}$, with which the confidence regions can be
calculated, as explained in Section 2.3.

Some applications of correspondence regression have shown that the\\*
\texttt{corregp()} function is not always able to handle very large data
sets well. Consequently, there is now the function \texttt{corregplicate()}
which repeats \texttt{corregp()} a number of times specified by
the extra argument \texttt{r} (in other words,\\*\texttt{corregplicate()}
is essentially a wrapper for \texttt{replicate(r, corregp, ...)}). That
means, of course, that the number of replications/simulations in e.g.
\texttt{corregplicate(..., b = 1000, r = 5, ...)} is equal to that of
\\*\texttt{corregp(..., b = 5000)}. Therefore, one can use
\texttt{corregplicate()} when certain (large) values of \texttt{b} present
problems for \texttt{corregp}: it suffices to reduce \texttt{b} and choose
a value for \texttt{r} so that their product equals the desired number of
replications/simulations (see \texttt{help(corregplicate)}).

\section*{Bibliography}

Adler, D., D. Murdoch and others (2017)
\emph{rgl: 3D Visualization Using OpenGL}. R package version 0.98.1.\\*
https://CRAN.R-project.org/package=rgl.
  
Alvarez, R., M. Becue, J.J. Lanero and O. Valencia (2002) Results
stability in textual analysis: Its application to the study of Spanish
investiture speeches (1979-2000).
\emph{Proceedings of the Journees internationales d'Analyse statistique des
Donnees Textuelles 2002}, 1--12.

Alvarez, R., M. Becue and O. Valencia (2004) Etude de la stabilite des
valeurs propres de l'AFC d'un tableaux lexical au moyen de procedures de
reechantillonage.
\emph{Proceedings of the Journees internationales d'Analyse statistique des
Donnees Textuelles 2004}, 42--51.

Alvarez, R., M. Becue and O. Valencia (2006) Partial bootstrap in CA:
correction of the coordinates. Application to textual data.
\emph{Proceedings of the Journees internationales d'Analyse statistique des
Donnees Textuelles 2006}, 43--53.

Darroch, J.N. (1974) Multiplicative and additive interaction in
contingency tables. \emph{Biometrika} \textbf{61} (2), 207--214.

Escofier, B. (1984) Analyse factorielle en reference a un modele:
Application a l'analyse de tableaux d'echanges.
\emph{Revue de Statistique Appliquee} \textbf{32} (4), 25--36.

Fisher, R.A. (1940) The precision of discriminant functions.
\emph{Annals of Eugenics} \textbf{10} (1), 422--429.

Gilula, Z. and S.J. Haberman (1988) The analysis of multivariate
contingency tables by restricted canonical and restricted association
models. \emph{Journal of the American Statistical Association}
\textbf{83} (403), 760--771.

Greenacre, M. (2017)
\emph{Correspondence analysis in practice, Third edition}. Boca Raton:
Chapman and Hall/CRC.

Hirschfeld, H. O. (1935) A connection between correlation and contingency.
\emph{Proceedings of the Cambridge Philosophical Society}
\textbf{31} (4), 520--524.

Kroonenberg, P.M. and C.J. Anderson (2006) Additive and multiplicative
models for three-way contingency tables: Darroch (1974) revisited. In:
M. Greenacre and J. Blasius (eds),
\emph{Multiple Correspondence Analysis and Related Methods}. Boca Raton:
Chapman and Hall/CRC, 455--502.

Lebart, L. (2004) Validite des visualisations de donnees textuelles.
\emph{Proceedings of the Journees internationales d'Analyse statistique des
Donnees Textuelles 2004}, 708--715.

Murdoch, D. and E.D. Chow (2013)
\emph{ellipse: Functions for drawing ellipses and ellipse-like confidence
regions}. R package version 0.3-8.\\*
https://CRAN.R-project.org/package=ellipse.

Soetaert, K. (2014)
\emph{diagram: Functions for visualising simple graphs (networks), plotting
flow diagrams}. R package version 1.6.3.\\*
https://CRAN.R-project.org/package=diagram.

Van der Heijden, P.G.M., A. De Falguerolles and J. De Leeuw (1989) A
combined approach to contingency table analysis using correspondence
analysis and log-linear analysis. \emph{Applied Statistics} \textbf{38} (2),
249--292.

Venables, W.N. and B.D. Ripley (2002)
\emph{Modern applied statistics with S}. New York: Springer.

\end{document}